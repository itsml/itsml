{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability estimation for an LSTM network with Platt Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sindelar, L. (2020). *Time Series and Confidence Analysis for Remaining Useful Lifetime*. Unpublished Master's Thesis, Bielefeld University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Collection for data import and evaluation functions\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario:\n",
    "\n",
    "An airplane company is investigating the lifetime of their jet engines to optimise their maintenance process.\n",
    "\n",
    "The company collected data from previously run enginges. The sensors are queried in intervals and inlcude:\n",
    "* temperatures\n",
    "* pressures\n",
    "* ratios (e.g. of fuel to air)\n",
    "* ...\n",
    "\n",
    "The machine learning model is trained to estimate whether an engine will fail in the next $w$ intervals(cycles). \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c(x) &=\\begin{cases}\n",
    "\t\t\t1 &, \\text{RUL}(x) \\le w\\\\\n",
    "\t\t\t0 &, \\text{else}\n",
    "\t\t\t\\end{cases}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "We use the NASA [Turbofan data set](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan) for the purpose of this scenario. \n",
    "\n",
    "To feed the data into the model, sequences with a set length were created overlappingly using a sliding window.\n",
    "For example, let's assume we want a sequence length of 3. Then a series of length 10 is cut into 8 sequences of length 3 (see run A below). However, if the series is shorter than the sequence length, it is disregarded (see run B below).\n",
    "\n",
    "![sequences](img/sequences.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Splitting the data:\n",
    "Furthermore, we split the training data into a training and a validation data set. The validation data set is then used for the Platt scaling to prevent unwanted bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes data is labeled as follows:\n",
    "# train_FD001.txt\n",
    "# test_FD001.txt\n",
    "# RUL_FD001.txt\n",
    "data_path = \"data/\"\n",
    "data_idx = 1\n",
    "\n",
    "# Define that all columns are used for training\n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting_1', 'setting_2', 'setting_3', 'time_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# Load data\n",
    "data = CMAPSS_Data(data_path=data_path)\n",
    "train_df, test_df = data.load_data(data_idx, w=30, preprocess=\"std\")\n",
    "\n",
    "# Create sequences\n",
    "seq_dict = data.create_sequences(train_df, test_df, 'label_1', sequence_cols,\n",
    "                                 val_ratio=0.4, sequence_length=50, save=False)\n",
    "# Training set sequences\n",
    "seq_array =seq_dict['train_seq']\n",
    "label_array =seq_dict['train_label']\n",
    "# Validation set sequences\n",
    "seq_array_val =seq_dict['val_seq']\n",
    "time_array_val =seq_dict['val_time']\n",
    "label_array_val =seq_dict['val_label']\n",
    "# Test set sequences\n",
    "seq_array_test =seq_dict['test_seq']\n",
    "time_array_test =seq_dict['test_time']\n",
    "label_array_test =seq_dict['test_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "In this notebook we use an LSTM network. LSTM networks offer good reliability on time series data. The keras implementation also supports sequences as input.\n",
    "\n",
    "We are use the following structure:\n",
    "\n",
    "![lstm](img/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 50, 100)           50400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 80,651\n",
      "Trainable params: 80,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model parameters\n",
    "sequence_length = 50\n",
    "batch_size = 200\n",
    "epochs = 100\n",
    "model_path = 'model/its_ml_model_'+ str(data_idx) + '.h5'\n",
    "\n",
    "# Derive input and output dimensions\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the network\n",
    "selected_callbacks = [callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                      callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "\n",
    "history = model.fit(seq_array, label_array, epochs=epochs, batch_size=batch_size, validation_split=0.05, verbose=0,\n",
    "          callbacks= selected_callbacks)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platt Scaling\n",
    "\n",
    "Platt scaling is a method for probability estimation for non-probabilistic models by Platt (1999). It was originally developed for the SVM, but is also applicable for artificial neural networks like the LSTM (see Niculescu-Mizil and Caruana, 2005). It takes the raw model output and fits it to a sigmoid function. This sigmoid function can then be used for the estimation of class probabilities.\n",
    "\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "Minimise the negative log likelihood of the training data ($f_i, y_i$) for the parameters of the sigmoid function $A$ and $B$:\n",
    "\n",
    "$$\n",
    "\\argmin\\limits_{A, B} - \\sum_{i} y_i log(p_i) + (1 - y_i) log(1 - p_i)\n",
    "$$\n",
    "\n",
    "where the probability $p_i$ is defined by the sigmoid function:\n",
    "$$\n",
    "p_i = \\frac{1}{1 + exp(A f_i + B)}\n",
    "$$\n",
    "\n",
    "For the LSTM network we use the second to last layer as input for the Platt scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer data function\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer(index=5).output)\n",
    "\n",
    "clf_out = intermediate_layer_model.predict(seq_array_val)\n",
    "\n",
    "# Apply Platt scaling\n",
    "\n",
    "target = label_array_val\n",
    "prior_1 = np.count_nonzero(target)\n",
    "prior_0 = target.shape[0] - prior_1\n",
    "\n",
    "# The line search often fails, but still produces a good result\n",
    "A, B = newton_platt(clf_out, target, prior_1, prior_0, minstep=1e-15)\n",
    "\n",
    "# Use Platt scaling for test set\n",
    "test_clf_out = intermediate_layer_model.predict(seq_array_test)\n",
    "pred_probs = predict_platt_probs(A, B, test_clf_out)\n",
    "print(\"Sigmoid A={}, B={}\".format(A, B))\n",
    "\n",
    "plt.scatter(test_clf_out, pred_probs, s=1)\n",
    "plt.ylabel('pred_probs')\n",
    "plt.xlabel('model output')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(time_array_test, pred_probs, s=1)\n",
    "plt.ylabel('pred_probs')\n",
    "plt.xlabel('time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result:\n",
    "\n",
    "In the first plot we can see the training of a sigmoid to the model output was successful. The sigmoid shapes of the second plot are shifted, because the sigmoid output is plotted against the time and not the model output.\n",
    "\n",
    "With the estimated probabilities we get an idea of how the certain an engine failure is for each model output. The estimation for each time step is less precise, since the time series vary in length. This can be used in further steps to make decisions about maintainance schedules and other decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now it has to be looked into how well the model performes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_lstm(model, seq_array, label_array, title='Training', verbose=True)\n",
    "\n",
    "evaluate_lstm(model, seq_array_test, label_array_test, title='Test', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* C-MAPSS data set: [data set](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan) and [publication](https://ti.arc.nasa.gov/m/pub-archive/154/154_Saxena.pdf)\n",
    "* Based on code by [Umberto Griffo](https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM)\n",
    "\n",
    "## Literature:\n",
    "* Saxena et al. \"Damage propagation modeling for aircraft engine run-to-failure simulation\". In: 2008 International Conference on Prognostics and Health Management (2008).\n",
    "* Platt. \"Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods\". In: ADVANCES IN LARGE MARGIN CLASSIFIERS (1999).\n",
    "* Niculescu-Mizil and Caruana. \"Predicting Good Probabilities With Supervised Learning\". In: ICML 05: Proceedings of the 22nd international conference on Machine learning (2005).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its-ml",
   "language": "python",
   "name": "its-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
